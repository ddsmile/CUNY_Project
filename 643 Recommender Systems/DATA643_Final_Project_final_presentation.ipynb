{"cells":[{"cell_type":"markdown","source":["#DATA643 - Final Project\n### Prashanth Padebettu, Parshu Rath, Adejare Windokun, Xingjia Wu\n##### Summer 2016 \n##### Instructor: Andrew Catlin"],"metadata":{}},{"cell_type":"markdown","source":["## Objectives"],"metadata":{}},{"cell_type":"markdown","source":["The main objective of this project is to build a context-aware recommender system using the Movielens dataset on Apache Spark cluster system. The goal is to incorporate contextual information (such as Time) in order to deliver not just movie recommendations but also unique user experiences shaped by user context, and take advantage of the distributed computing power to handle processing of large scale data and thus increasing relevance of the recommendation models to the users by reducing latency."],"metadata":{}},{"cell_type":"markdown","source":["### Description of workflows:\n1. Loading the data - the data was loaded by importing the 3 files and merged into a Pandas Dataframe.\n\n2. Data exploration - The dataframe was converted to a Spark DataFrame object that allowed us to perform data using SQL.\n\n3. Data visualization - Dataframe into a Resilient Distributed Dataset (RDD) to demonstrate how visualizations can be performed on large datasets.\n\n4. Machine Learning, Recommendations and Predictions were performed using the Spark MLlib library with the Alternating Least Squares method. Models were evaluated based on RMSE. \n\n5. Contextual pre-filtering technique was enabled before training and testing the models. Contexts included the day of the week and time of the day that the user wished to watch a movie\n\n6. Add new user ratings and re-train the model with combined dataset. Re-run the model to make recommendations and predict individual ratings\n\n7. We also compared the results of using the 1,000,000 records as compared to using a much smaller set of records 100,000"],"metadata":{}},{"cell_type":"code","source":["# Required Python Packages\nfrom time import time\n\ntstart = time()\n\nfrom math import sqrt\nimport pandas as pd\nimport numpy as np\nfrom scipy import spatial\nimport scipy \nimport matplotlib.pyplot as plt\nimport datetime\nimport os\nfrom operator import add"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Create dataframe from 1M movielens data"],"metadata":{}},{"cell_type":"markdown","source":["The Movielens dataset is available at http://grouplens.org/datasets/movielens/, and consist of three text files containing data on 1 million ratings from 6000 users on 4000 movies, and was released in 2003 (movielens.com).\n\nThe files include the following:\n\n1. Ratings.dat consisting of userid, movieid, timestamp and ratings (between 1 to 5)\n\n2. Users.dat consisting of demographic data on the users\n\n3. Movies.dat consisting of movieid, movietitle and genres of movies\n\nThese 3 files where download and unzipped and stored in Github for use in the application"],"metadata":{}},{"cell_type":"code","source":["# loading 1,000, 000 Movie lens data directly from github\npath = \"https://raw.githubusercontent.com/ppadebettu/CUNY/Master/IS_643_Recommender_Systems/Final_Project/Data/\"\nmovies_fname = 'movies.dat'\nurl = path + movies_fname\npd_movies = pd.read_csv(url, sep = '::', engine='python', header = None, na_values='NaN', usecols = [0,1], \n                         names = ['movieid', 'movietitle'])\n\nusers_fname = 'users.dat'\nurl = path + users_fname\npd_users = pd.read_csv(url, sep = \"::\" , engine='python', header = None, na_values='NaN',\n                        names = ['userid', 'gender', 'age', 'occupation', 'zipcode'])\n\nratings_fname = 'ratings.dat'\nurl = path + ratings_fname\n\npd_ratings = pd.read_csv(url, sep = \"::\" , engine='python', header = None, na_values='NaN', \n                        names = ['userid', 'movieid', 'rating', 'timestamp'])\n\n# Merge three dataframe into one\nresult = pd.merge(pd_ratings,pd_movies, on = ['movieid'] )\nmovielens = pd.merge(result,pd_users, on = ['userid'] )\n\n\n# Code to add the new columns that will contain the context data that is obtained from the timestamp\ndef fdate(x):   \n    return datetime.datetime.fromtimestamp(\n        int(str(x['timestamp']))).strftime('%Y-%m-%d') \n\ndef ftime(x):   \n    return datetime.datetime.fromtimestamp(\n        int(str(x['timestamp']))).strftime('%H:%M:%S') \n\ndef fweekday(x):   \n    \n    if (datetime.datetime.fromtimestamp(int(str(x['timestamp']))).weekday() >= 4):\n        return 'Weekend'\n    else:\n        return 'Weekday'\n    \ndef fagegroup(x):   \n    \n    if (x['age'] >= 45):\n        return '45+'\n    \n    elif (x['age'] >= 30):\n        return '30-44'\n    \n    elif (x['age'] >= 19):\n        return '19-29'\n    else:\n        return 'below 18' \n    \ndef ftimeofday(x): \n    \n    t = datetime.datetime.fromtimestamp(int(str(x['timestamp']))).strftime('%H:%M:%S')\n    \n    if (t >= '23:00:00'):\n        return 'night'\n    \n    elif (t >= '18:00:00'):\n        return 'evening'\n    \n    elif (t >= '12:00:00'):\n        return 'afternoon'\n    \n    elif (t >= '08:00:00'):\n        return 'morning'\n    \n    else:\n        return 'night'\n    \ndef flocation(x):   \n    \n    start = datetime.datetime.strptime(x['date'], '%Y-%m-%d')\n    end = datetime.datetime.strptime(x['releasedate'], '%d-%b-%Y')\n    \n    if start - end >= datetime.timedelta(180):\n        return 'home'\n    else:\n        return 'theater'       \n    \n    \nmovielens['date'] = movielens.apply(fdate, axis=1)\nmovielens['time'] = movielens.apply(ftime, axis=1)\nmovielens['weekday'] = movielens.apply(fweekday, axis=1)\nmovielens['timeofday'] = movielens.apply(ftimeofday, axis=1)\n   \n"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["__Movielens Dataset Dimension (rowsXcolumns)__"],"metadata":{}},{"cell_type":"code","source":["movielens.shape"],"metadata":{"collapsed":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#movielens.head()\nmovielens.iloc[np.random.random_integers(1000209, size=5)]"],"metadata":{"collapsed":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["__Create CSV and Save to Local Databricks Cloud (optional)__"],"metadata":{}},{"cell_type":"code","source":["# dbutils.fs.mkdirs(\"/moviedat/\")\n# display(dbutils.fs.ls(\"dbfs:/moviedat/\"))\n# #Remove the file if exists\n# dbutils.fs.rm(\"dbfs:/moviedat/movielens.csv\")\n# #Save to csv file\n# movielens.to_csv(\"/dbfs/moviedat/movielens.csv\", header=True, index=False) # 93M\n# display(dbutils.fs.ls(\"dbfs:/moviedat/\"))\n# # #Load csv file as Spark-RDD\n# #movie_rdd = sc.textFile(\"dbfs:/movielens.csv\")\n# # header=movie1.first()\n# # movie_rdd_split = movie1.filter(lambda x: x != header).map(lambda l: l.split(\",\"))\n# # movie_rdd_split.take(5)\n# #Load csv file as Spark-SQLdf\n# sqlCtx = sqlContext.getOrCreate(sc)\n# movie_df = sqlCtx.read.format('com.databricks.spark.csv').options(header='true').load(\"dbfs:/movielens.csv\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["### Data Exploration and Visualization"],"metadata":{}},{"cell_type":"markdown","source":["#### - Create Spark DataFrame"],"metadata":{"collapsed":true}},{"cell_type":"code","source":["sqlCtx = sqlContext.getOrCreate(sc)\nmovie_df = sqlCtx.createDataFrame(movielens)"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["#movie_df.show(5)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["#### - Data exploration"],"metadata":{}},{"cell_type":"code","source":["# Register as temp table in sqlContext\nmovie_df.registerTempTable(\"ratings_data_all\")"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["- __Summary of data__"],"metadata":{}},{"cell_type":"code","source":["#Display summary from the table\nmovie_df.select(\"rating\", \"age\").describe().show()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["- __Top 10 users with most ratings__"],"metadata":{}},{"cell_type":"code","source":["#SQL can be run over DataFrames that have been registered as a table. \n#Top 10 users with most ratings\nsqlContext.createDataFrame(sqlContext.sql(\"SELECT userid, COUNT(*) AS ratings_count FROM ratings_data_all GROUP BY userid \\\nORDER BY ratings_count DESC LIMIT 10\").collect()).show()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["- __Number of movies by weekday and weekend__"],"metadata":{}},{"cell_type":"code","source":["# Number of movies by weekday and weekend\nsqlContext.createDataFrame(sqlContext.sql(\"SELECT weekday, COUNT(distinct(movieid)) AS movie_count FROM ratings_data_all GROUP BY weekday\").collect()).show()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["- __Number of movies watched by time of day__"],"metadata":{}},{"cell_type":"code","source":["# Number of movies watched by time of day\nsqlContext.createDataFrame(sqlContext.sql(\"SELECT timeofday, COUNT(distinct(movieid)) AS movie_count FROM ratings_data_all GROUP BY timeofday\").collect()).show()"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["####- Visualization"],"metadata":{}},{"cell_type":"code","source":["# select needed columns\nm_v = movie_df.map(lambda x: (x[0], x[2], x[4], x[5], x[11], x[12]))\nsqlContext.createDataFrame(m_v.take(3), schema=('userid', 'rating', 'movietitle', 'genger', 'weekday', 'timeogday')).show()\n#schema=('userid', 'rating', 'movietitle', 'genger', 'weekday', 'timeogday')\n"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Function to show histogram\ndef histogram(movies):\n    count = map(lambda x: x[0], movies)\n    movietitle = map(lambda x: x[1], movies)\n    plt.barh(range(len(count)), count, color = 'blue')\n    plt.yticks(range(len(count)), movietitle)\n    f = plt.gcf()\n    f.set_size_inches(16, 10)\n    display(f)\n"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["- __Most watched movies__"],"metadata":{}},{"cell_type":"code","source":["x = m_v.map(lambda x: (x[2]))\nx = x.map(lambda w: (w, 1))\nx = x.reduceByKey(add)\nx = x.map(lambda w: (w[1], w[0])).sortByKey(False)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":["# Most Watched Movies\nhistogram(x.take(5))"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["- __Ratings for movies__"],"metadata":{}},{"cell_type":"code","source":["# Bar Chart of Ratings for Moviesfig, ax = plt.subplots()\n\nfig, ax = plt.subplots()\nplt.hist(m_v.map(lambda x: (x[1])).collect(), facecolor='green', alpha=0.5)\ndisplay(fig)\n"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["- __Ratings by user__"],"metadata":{}},{"cell_type":"code","source":["# Ratings by User\n\nx = m_v.map(lambda x: (x[0]))\nx = x.map(lambda w : (w, 1))\nx = x.reduceByKey(add)\nx = x.map(lambda w: (w[1], w[0])).sortByKey(False)\n\nx.take(5)\nhistogram(x.take(25))"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["- __Movie watching (weekday and weekend)__"],"metadata":{}},{"cell_type":"code","source":["x = m_v.map(lambda x: (x[4]))\nx = x.map(lambda w: (w, 1))\nx = x.reduceByKey(add)\nx = x.map(lambda w: (w[1], w[0])).sortByKey(False)\n# The slices will be ordered and plotted counter-clockwise.\nlabels = x.map(lambda w: w[1]).collect()\nsizes =  x.map(lambda w: w[0]).collect()\ncolors = ['yellowgreen', 'gold']\nexplode = (0, 0.1,)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig, ax = plt.subplots()\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\n        autopct='%1.1f%%', shadow=True, startangle=90)\n# Set aspect ratio to be equal so that pie is drawn as a circle.\nplt.axis('equal')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["- __Movie watching (Time of the day)__"],"metadata":{}},{"cell_type":"code","source":["x = m_v.map(lambda x: (x[5]))\nx = x.map(lambda w: (w, 1))\nx = x.reduceByKey(add)\nx = x.map(lambda w: (w[1], w[0])).sortByKey(False)\n\n# The slices will be ordered and plotted counter-clockwise.\nlabels = x.map(lambda w: w[1]).collect()\nsizes =  x.map(lambda w: w[0]).collect()\ncolors = ['yellowgreen', 'gold', 'lightskyblue', 'lightcoral']\nexplode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig, ax = plt.subplots()\nplt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=90)\n#Set aspect ratio to be equal so that pie is drawn as a circle.\nplt.axis('equal')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["### Recommendation"],"metadata":{}},{"cell_type":"markdown","source":["__Save movie titles__"],"metadata":{}},{"cell_type":"code","source":["#Get movie titles\nmovies_titles = movielens[['movieid', 'movietitle']]\nmovies_titles =movies_titles.drop_duplicates()\nprint movies_titles.shape\nmovies_titles.head()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"code","source":["# Convert to RDD\n#Get movie titles\nmovies = sqlCtx.createDataFrame(movies_titles)\nprint movies.take(3)\nm = movies.map(lambda x: (int(x[0]),x[1]))\n"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["#### - Create RDD"],"metadata":{}},{"cell_type":"code","source":["movie_RDD = movie_df.rdd"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["#movie_RDD.take(3)\nsqlContext.createDataFrame(movie_RDD.take(3)).show()"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["#### - Context prefiltering"],"metadata":{}},{"cell_type":"markdown","source":["__Prefilter with \"Weekday\" and \"evening\" as our example__"],"metadata":{}},{"cell_type":"code","source":["# Prefilter\nprefilter = movie_RDD.filter(lambda x: x[11]=='Weekday' and x[12]=='evening')"],"metadata":{},"outputs":[],"execution_count":51},{"cell_type":"code","source":["#prefilter.take(3)\nsqlContext.createDataFrame(prefilter.take(3)).show()\n"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"code","source":["prefilter.count()"],"metadata":{},"outputs":[],"execution_count":53},{"cell_type":"markdown","source":["#### - Data split into Training, Validation and Test RDD"],"metadata":{}},{"cell_type":"code","source":["# Get only userid, movieid and rating\nmovie_rdd = prefilter.map(lambda x: (x[0], x[1], int(x[2])))"],"metadata":{},"outputs":[],"execution_count":55},{"cell_type":"code","source":["movie_rdd.take(3)"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"code","source":["#Split data into training and test datasets\ntraining_RDD, validation_RDD, test_RDD = movie_rdd.randomSplit([6, 2, 2], seed=0L)\nvalidation_for_predict_RDD = validation_RDD.map(lambda x: (x[0], x[1]))\ntest_for_predict_RDD = test_RDD.map(lambda x: (x[0], x[1]))"],"metadata":{},"outputs":[],"execution_count":57},{"cell_type":"code","source":["#test_for_predict_RDD.take(5)\ntraining_RDD.take(3)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["####- Collaborative Filtering implementation by using Alternating Least Squares"],"metadata":{}},{"cell_type":"markdown","source":["(Partially adapted from: https://github.com/jadianes/spark-movie-lens/blob/master/notebooks/building-recommender.ipynb )\n\nSpark MLlib library for Machine Learning provides a Collaborative Filtering implementation by using Alternating Least Squares. <br>\n\nThe implementation in MLlib has the following parameters:<br>\n\nnumBlocks is the number of blocks used to parallelize computation (set to -1 to auto-configure).<br>\nrank is the number of latent factors in the model.<br>\niterations is the number of iterations to run.<br>\nlambda specifies the regularization parameter in ALS.<br>\nimplicitPrefs specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data.<br>\nalpha is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.recommendation import ALS\nimport math\n\nseed = 5L\niterations = 10\nregularization_parameter = 0.1\nranks = [4, 8, 12]\nerrors = [0, 0, 0]\nerr = 0\ntolerance = 0.02\n\nmin_error = float('inf')\nbest_rank = -1\nbest_iteration = -1\nfor rank in ranks:\n    model = ALS.train(training_RDD, rank, seed=seed, iterations=iterations,\n                      lambda_=regularization_parameter)\n    predictions = model.predictAll(validation_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\n    ratesAndpreds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\n    error = math.sqrt(ratesAndpreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n    errors[err] = error\n    err += 1\n    print 'For rank %s the RMSE is %s' % (rank, round(error,4))\n    if error < min_error:\n        min_error = error\n        best_rank = rank\n\nprint 'The best model was trained with rank %s' % best_rank"],"metadata":{"collapsed":false},"outputs":[],"execution_count":61},{"cell_type":"code","source":["#Let's check our predictions\npredictions.take(3)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":62},{"cell_type":"code","source":["#Let's compare predictions vs actuals (ratings)\n#ratesAndpreds.take(10)\nx = ratesAndpreds.map(lambda w: (w[0][0], w[0][1], w[1][0], round(w[1][1], 2)))\n#x.take(5)\nsqlContext.createDataFrame(x.take(10), schema=('userid', 'movieid', 'actual', 'predicted')).show()\n"],"metadata":{"collapsed":false},"outputs":[],"execution_count":63},{"cell_type":"code","source":["#Let's test the selected model\nmodel = ALS.train(training_RDD, best_rank, seed=seed, iterations=iterations,\n                      lambda_=regularization_parameter)\npredictions = model.predictAll(test_for_predict_RDD).map(lambda r: ((r[0], r[1]), r[2]))\nratesAndpreds = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions)\nerror = math.sqrt(ratesAndpreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n    \nprint 'For testing data (from 1M data) the RMSE is %s' % (round(error,4))"],"metadata":{"collapsed":false},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["### Add new user into dataset and test recommendation model"],"metadata":{}},{"cell_type":"code","source":["#Create a list of ratings for a new user (998)\nnew_user_ID_1 = 70000\n\n# The format of each line is (userID, movieID, rating)\nnew_user_ratings_1 = [\n (70000, 242, 4), # Kolya (1996)\n (70000, 51, 3),  # Legends of the Fall (1994)\n (70000, 465, 1),  # Jungle Book, The (1994)\n (70000 , 86, 2), # Remains of the Day, The (1993)\n (70000, 222, 3), # Star Trek: First Contact (1996)\n (70000, 274, 4), # Sabrina (1995)\n (70000, 1042, 3),  # Just Cause (1995)\n (70000, 1184, 3), # Endless Summer 2, The (1994)\n (70000, 265, 2), # Hunt for Red October, The (1990)\n (70000, 302, 3) # L.A. Confidential (1997)\n]\nnew_user_ratings_RDD_1 = sc.parallelize(new_user_ratings_1)\nprint 'New user ratings: %s' % new_user_ratings_RDD_1.take(10)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":66},{"cell_type":"code","source":["# Merge new user ratings to the existing RDD\ndata_with_new_ratings_RDD = movie_rdd.union(new_user_ratings_RDD_1)"],"metadata":{},"outputs":[],"execution_count":67},{"cell_type":"code","source":["#Train the ALS model using new dataset and all the parameters we selected before\nfrom time import time\n\nt0 = time()\nnew_ratings_model = ALS.train(data_with_new_ratings_RDD, best_rank, seed=seed, \n                              iterations=iterations, lambda_=regularization_parameter)\ntt = time() - t0\n\nprint \"New model trained in %s seconds\" % round(tt,3)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["####- Getting recommendations"],"metadata":{}},{"cell_type":"code","source":["#Getting top recommendations\nnew_user_ratings_ids = map(lambda x: x[1], new_user_ratings_1) # get just movie IDs\n# keep just those not on the ID list\nnew_user_unrated_movies_RDD = movie_rdd.filter(lambda x: x[0] not in new_user_ratings_ids)\\\n                               .map(lambda x:(new_user_ID_1, x[0]))\n\n# Use the input RDD, new_user_unrated_movies_RDD, with new_ratings_model.predictAll() to predict new ratings for the movies\nnew_user_recommendations_RDD = new_ratings_model.predictAll(new_user_unrated_movies_RDD)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":70},{"cell_type":"code","source":["# Transform new_user_recommendations_RDD into pairs of the form (Movie ID, Predicted Rating)\n# Use distinct() here\nnew_user_recommendations_rating_RDD = new_user_recommendations_RDD.distinct().map(lambda x: (x.product, x.rating))\nnew_user_recommendations_rating_RDD.take(3)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":71},{"cell_type":"code","source":["#Get movie titles\nmovies_titles = movie_RDD.map(lambda x: (int(x[1]),x[4]))\nmovies_titles.take(3)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":72},{"cell_type":"code","source":["#Merge movie titles and recommendations for the new user so that the results are meaningful\n#movies_titles = ratings_data.map(lambda x: (int(x[0]),x[1]))\nnew_user_recommendations_rating_title_RDD = new_user_recommendations_rating_RDD.join(movies_titles)\nnew_user_recommendations_rating_title_RDD.distinct().sortBy(lambda x: x[1][0], ascending=False).take(3)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":73},{"cell_type":"markdown","source":["### Sampling data and comparison between 1M and 100K\n\nCollaborative context filtering ALS model was also built with the data from 100K ratings data obtained fromm the 1M dataset. Prefilter was applied on the 1M dataset and 1/10th of the filtered data (approximately from 100K ratings data dfrom the 1M dartaset) was used to build an ALS model. RMSE of this model was evaluated and compared with the 1M filtered data."],"metadata":{}},{"cell_type":"code","source":["#Sample 1/10 of data (~100K) of the context filtered data\nmovie_rdd100 = movie_rdd.sample(False, 0.1, 3045)\n\nprint \"Total context filtered movies from 1M data set: %s \\\n\\nOne-tenth of the filtered movies:  %s\" % (movie_rdd.count(), movie_rdd100.count())\nmovie_rdd100.take(3)"],"metadata":{},"outputs":[],"execution_count":75},{"cell_type":"code","source":["#Split data into training and test datasets\ntraining_RDD100, validation_RDD100, test_RDD100 = movie_rdd100.randomSplit([6, 2, 2], seed=0L)\nvalidation_for_predict_RDD100 = validation_RDD100.map(lambda x: (x[0], x[1]))\ntest_for_predict_RDD100 = test_RDD100.map(lambda x: (x[0], x[1]))\n\n#Collaborative Filtering with ALS\nseed = 5L\niterations = 10\nregularization_parameter = 0.1\nranks = [4, 8, 12]\nerrors = [0, 0, 0]\nerr = 0\ntolerance = 0.02\nmin_error = float('inf')\nbest_rank = -1\nbest_iteration = -1\nfor rank in ranks:\n    model100 = ALS.train(training_RDD100, rank, seed=seed, iterations=iterations,\n                      lambda_=regularization_parameter)\n    predictions100 = model100.predictAll(validation_for_predict_RDD100).map(lambda r: ((r[0], r[1]), r[2]))\n    ratesAndpreds = validation_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions100)\n    error100 = math.sqrt(ratesAndpreds.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n    errors[err] = error100\n    err += 1\n    print 'For rank %s the RMSE is %s' % (rank, round(error100, 4))\n    if error < min_error:\n        min_error = error100\n        best_rank100 = rank\nprint 'The best model was trained with rank %s' % best_rank100\n"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"code","source":["#Test the selected model with test set data\npredictions100 = model100.predictAll(test_for_predict_RDD100).map(lambda r: ((r[0], r[1]), r[2]))\nratesAndpreds100 = test_RDD.map(lambda r: ((int(r[0]), int(r[1])), float(r[2]))).join(predictions100)\nerror = math.sqrt(ratesAndpreds100.map(lambda r: (r[1][0] - r[1][1])**2).mean())\n    \nprint 'For testing data (from 100 K) the RMSE is %s' % (round(error,4))\n"],"metadata":{},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":["####Model comparison\n\nThe 100K dataset had the filtered data with 18882 ratings. ALS model built based on this dataset had an RMSE of 1.405 and the test set RMSE of 1.328.  On the other hand, the ALS model built with all the filtered data (190150 ratings) from the 1M dataset had an RMSE of 0.916 and the test set RMSE of 0.918. This shows that the model improves as the number of input ratings is increased."],"metadata":{}},{"cell_type":"markdown","source":["### Discussion\n\nLarge datasets become increasing difficult to process in a timely manner using standard data analytic techniques. Due to this, specialized software and hardware have been developed to allow this processing to occur in a much faster time.\n\nDistributed computing which allows data to be distributed to different clusters and similarly processed by different clusters has been found to vastly reduce processing times. \n\nThe Spark engine on Python, which we chose is one of such applications and it allows the programmer to work with a programing language that is familiar to them (Python in this case), while the implementation of the clustering, data sharing and computation is hidden from them.\n\nSpark can be run on Java, Python, R and Scala, but for this exercise we chose Python. Spark can also be run locally on a single computer, on a virtual machine using Linux, or in the cloud, either as a hosted machine (AWS) or as a hosted platform (DataBricks).\n\n__Problems and Issues__  \n\n_a)\tLocal Machine_   \n\nWe attempted to use Pyspark (Python implementation of Spark) on our local machines after installing Spark and Java. However, the processing was unstable and we kept on getting multiple errors. Pyspark was impossible to run with a dataset that contained more than 100,000 records. In addition, each local machine had a different version of Python, Java and Spark installed with different configurations which made it difficult to troubleshoot.   \n<br>\n\n_b)\tAWS_   \n\nWe attempted to use AWS. AWS was also unstable, probably due to environment configuration issues.We kept on getting multiple errors with configurations and loss of connectivity. In the EC2 instance, we were able to setup a master and two slave nodes and after much effort and Linux configurations, we were able to connect from Jupyter on our local machine to the spark instance in AWS. The stability of the EC2 instance particularly the spark nodes were not sustainable, as the code that ran fine in one instance wiould give JVM memory issues in successive runs. Some of the stage tasks would try to finish up even as the worker tasks are still completing their work.  Majority of the issues were related to socket exceptions and JVM memory issues. This prevented us from focusing on the actual code as most of the time were lost to troubleshooting the configuration and memory issues. In addition to that, some of the group members (2) were not able to get AWS up and running at all.\n\n_c)\tVirtual Machine using Ubuntu 16.04_   \n\nWe installed a Virtual Machine using Oracle Virtual Box, with Ubuntu 16.04, Anaconda, and Spark, and this allowed us to use Jupyter as the front end. This worked perfectly and we were able to run our 1,000,000 record dataset without any issues at all. \n\nHowever, the Virtual Machine does require some technical expertise, and not all of our computers had the necessary hardware to allow installation of the Virtual Machine.\n\n_d)\tDataBricks_   \n\nDatabricks (https://databricks.com) is a fully hosted Apache Spark Cluster that allows users to run Spark in Jupyter on the cloud without having to install anything on their local machines. Databricks provides a cluster of 6GB to each user for free. We decided to use this as it allowed all of us to have a single well defined environment for programming, code testing and more importantly for troubleshooting. One problem we noticed with the databricks is that, the created cluster terminates automatically after one hour of inactivity which can lead to excessive loss of time in reloading the dataset into the clusters.\n\nIn summary we used DataBricks which is a hosted Apache Spark cloud based platform for our project due to its ease of use ans environment stability. We did do some of the initial programing work on our local spark cluster, AWS and on the Virtual Ubuntu Server, but transferred all this to DataBricks for the final deployment"],"metadata":{}},{"cell_type":"markdown","source":["### Conclusion\n\nRecommender systems are becoming ubiquitous due to the large amount of possible choices, the increase in the ability to capture both implicit and explicit data about users and items and computing power. However, the large amount of data that is collected makes it very difficult to analyze this data in a timely manner. Distributed data storage and computing (cluster computing) has come to the rescue and allows large datasets to be computed much more efficiently. Spark allows the use of such cluster by allowing a similar user-friendly interface (Python in this instance) while it hides and takes care of the complexities of the cluster computing.\n\nHowever, as we found out this does come with a price, systems setup to run cluster computing have to be finely tuned in terms of software, hardware, memory, etc and they are not easy to setup or manage. We were not able to use AWS for our project. While Linux machines are probably easier to setup and use, they are not suitable for teams as Linux machines are not that common.\n\nHosted services such as DataBricks are probably preferred as they are already setup, do not require an installation, and they take care of all the management and configuration issues (zero management). They also allow the user to ?Launch, dynamically scale up or down, and terminate clusters with just a few clicks?\n\nIn conclusion, our project demonstrates the use of a large dataset to recommend and predict movies for users. We used Spark on Python as our Programming Language, and used a hosted (DataBricks) cluster environment to run our program."],"metadata":{}},{"cell_type":"code","source":["tend = time() - tstart\nprint \"Total computation time: %s\" %tend"],"metadata":{},"outputs":[],"execution_count":81}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2.0},"version":"2.7.12","nbconvert_exporter":"python","file_extension":".py"},"name":"DATA643_Final_Project_v6_xw (1)","notebookId":694827509906930,"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"anaconda-cloud":{}},"nbformat":4,"nbformat_minor":0}
